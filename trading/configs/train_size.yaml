training:
  epochs: 6
  batch_size: 128
  learning_rate: 5.0e-4
  grad_clip_norm: 1.0
  train_ontologies:
    - T
    - R
  max_missing_ratio: 0.10

loss:
  clip_return: 0.01
  cap_up: 0.005
  l2_reg: 1.0e-3
  entropy_reg: 1.0e-3

forward_proxy:
  horizon: 8

logging:
  dump_weights: true
  dump_grad_norms: true
