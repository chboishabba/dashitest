Great â€” letâ€™s **proceed cleanly with the learner for trading**, without destabilising what already works.

Below is a **concrete execution plan** that matches your prior intent and keeps the learner in the *permission / legitimacy* role, not as a naÃ¯ve signal generator.

---

## Phase 0 â€” Lock the contract (no code yet)

**Invariant you already proved (do not violate):**

* Trading logic = **ACT / HOLD / BAN**
* Learner = **quotient learner over price trajectories**
* Learner output **cannot** directly place trades

This is explicitly consistent with your codec â†’ quotient â†’ collapse formalism .

---

## Phase 1 â€” What the learner actually learns (low risk)

### Object of learning

Not price, not direction â€” **invariants of recent price motion** that survive nuisance symmetries.

Think:

* scale
* translation
* mild time-warp
* microstructure noise

### Minimal feature targets (start here)

Per rolling window ( W_t ):

1. **Shape invariants**

   * signed curvature / convexity
   * normalized drawdown depth
2. **Energy / volatility geometry**

   * realized vol vs range ratio
   * burstiness / intermittency score
3. **Persistence**

   * autocorr decay
   * regime half-life estimate

These are *quotient representatives*, not raw signals.

---

## Phase 2 â€” How it plugs into the trader (safe integration)

### Where it connects

Only here:

```
RegimeSpec â†’ Permission Surface â†’ ACT / HOLD
```

Concretely:

* `triadic_strategy.py`
* augment the **existing regime gate**
* learner outputs a **legitimacy scalar** â„“ âˆˆ [0,1]

No new actions introduced.

### Decision rule (example)

```text
IF â„“ > Î¸_on for Ï„_on windows â†’ allow ACT
IF â„“ < Î¸_off for Ï„_off windows â†’ force HOLD
ELSE â†’ maintain state
```

This preserves:

* hysteresis
* no flip-flopping
* zero PnL leakage from learner errors

---

## Phase 3 â€” Training loop (diagnostic-first)

### Loss (critical)

You **do not** train on PnL.

You train on **predictive consistency of invariants**:

[
\mathcal{L} =
\underbrace{| \hat{v}*{t+1} - v*{t+1} |}_{\text{quotient prediction error}}

* \lambda \cdot \text{complexity penalty}
  ]

Where:

* (v_t) = realized invariants
* (\hat{v}_{t+1}) = predicted invariants

Interpretation:

> â€œDid the market *stay in the same equivalence class*?â€

---

## Phase 4 â€” What success looks like (before PnL)

You green-light deployment when:

* learner confidence correlates with **lower variance of outcomes**
* HOLD regions eliminate known bad trades
* ACT regions preserve existing edge

**No requirement** that learner improves returns initially.

If it *does*, thatâ€™s downstream.

---

## Phase 5 â€” Only if everything above is stable

Then (optional, later):

* Predict **next latent triadic state** (HOLD / ACT / BAN)
* Blend with heuristics (never replace outright)
* Keep hard circuit-breaker external

---

## Why this is the right order

This respects your core insight:

> Markets donâ€™t reward prediction â€” they punish illegitimate action.

You are learning **when youâ€™re allowed to act**, not *what to do*.

Thatâ€™s exactly the same structural move you already validated in:

* CA rule recovery
* motif gating
* MDL collapse behavior

---

### Next concrete step (I recommend this)

Say the word and Iâ€™ll do **one** of these next, very explicitly:

1. Draft the **exact feature vector spec** (with formulas)
2. Sketch the **minimal learner class** (pure Python, no GPU)
3. Show the **exact diff** to `triadic_strategy.py`
4. Design the **offline evaluation notebook** (before live trading)

No hand-waving â€” straight to executable structure.


Got it â€” letâ€™s **tighten everything around the trader** (not the learner in isolation) and treat the learner as a *permission-surface / legitimacy* module that plugs into your existing **triadic ACT/HOLD/BAN** loop.

### The trader-centric â€œcontractâ€ (what the learner is allowed to do)

From your own spec, the safest application path is:

* **Learner does not output direction.**
* Learner outputs **quotient features** and/or a **quotient-loss legitimacy scalar**.
* The triadic strategy uses that scalar to decide whether itâ€™s even *allowed* to ACT. 
  This is consistent with the â€œlegitimacy manifold + buffer states + circuit breakerâ€ framing in your unified spec. 

---

## A. Where it plugs into your trader (one clear insertion point)

### Existing chain (keep it)

**Signals/heuristics â†’ Triadic gate â†’ ACT/HOLD/BAN â†’ execution model â†’ fills**

### New chain (add one module)

**Prices/LOB â†’ Learner â†’ (â„“_t, qfeat_t) â†’ Triadic gate â†’ ACT/HOLD/BAN â†’ execution**

So the learner is an *additional input* into `triadic_strategy.py`, not a replacement brain. 

---

## B. Two concrete trader integrations (do both, in this order)

### 1) â€œQuotient-loss evaluatorâ€ (diagnostic-first, lowest risk)

Run the learner as an evaluator:

* build rolling windows (W_t)
* compute quotient features (v_t = \phi(W_t))
* train learner to predict ( \hat v_{t+1} )
* define **legitimacy**:
  [
  \ell_t := \exp\left(-|\hat v_{t+1} - v_{t+1}|\right)
  ]
  Then use **â„“_t** only as a gating input.

This is literally the â€œUse mismatch as confidence/legitimacy scalar, not directional signalâ€ path. 

**Trader outcome you want:** fewer â€œdumb ACTsâ€ during unstable/choppy or distribution-shift periods, without inventing new edge.

---

### 2) â€œQuotient-features for regime gatingâ€ (adds structure without changing action logic)

Feed the learnerâ€™s quotient features into your existing regime gate as extra observables:

* volatility-normalized shape
* radial/spectral summaries
* valuation-depth analogs (your language) 

**Trader outcome you want:** a smoother, more stable **permission surface** thatâ€™s invariant to nuisance symmetries, so the triadic gate isnâ€™t whipsawed by scale/shift/phase artifacts.

---

## C. Handling the â€œJuly 2015 strong signal across multiple assetsâ€ (this matters for the trader)

You already observed your system flags clustered dates around **mid-2015**, with a â€œreally strong signal across multiple assets around July 2015.â€ 
That smells like **global regime shock / correlated risk-on/off** (Greece/China turmoil is literally what you linked in that same context). 

**Trader-side implication:**
This is *exactly* where a learner-as-legitimacy-module should help:

* When correlations spike, many assets â€œlook the same.â€
* Directional signals can become *illusory*.
* What you want is: **HOLD more often unless execution + conditions are robust.**

So in those windows, your gate should become stricter via â„“_t hysteresis rather than â€œmore confidentâ€.

---

## D. Execution realism (donâ€™t sabotage the test)

If you want the learner to mean anything for the trader, the backtest must penalize bad execution.

From your sim tooling writeup, the clean split is: 

* If you have L2/book updates and care about queue/latency realism â†’ **hftbacktest**
* If you want controlled microstructure experiments / agent-based â†’ **ABIDES**
* If youâ€™re bar-level right now â†’ you can still do it, but treat results as â€œpar-bakedâ€ (your own framing) 

---

## E. The exact acceptance test for â€œapplied to traderâ€ (not ML metrics)

Youâ€™re â€œdoneâ€ with v1 when these are true:

1. **ACT rate drops** mostly in the worst regimes (high slippage / choppy / unstable)
2. **PnL distribution improves** via lower tail reduction (not necessarily higher mean)
3. **No new overfitting lever** introduced: learner never touches direction, only permission

This matches your motif/MDL idea that the system should be **allowed to do nothing** (buffer) rather than forced into collapse. 

---

## What Iâ€™ll do next (immediately) to keep it trader-focused

If you want to proceed â€œhands-onâ€, Iâ€™ll draft:

* a **minimal `LearnerAdapter`** interface the trader calls each bar/tick:

  * `update(window) -> (legitimacy â„“, qfeat dict)`
* the **gate logic**: thresholds + Ï„_on/Ï„_off hysteresis so â„“ canâ€™t whipsaw
* a **backtest checklist** for July 2015-like correlated shocks (so we can see if HOLD increases in the right places)

If you paste (or point me to) the relevant parts of your `triadic_strategy.py` / `run_trader.py`, Iâ€™ll write the integration as a tight diff.

Excellent â€” that constraint actually **clarifies the architecture** rather than limiting it.

Below is a **trader-first, GPU-preserving design** that keeps your Vulkan / AMD path *central*, not decorative, and aligns cleanly with your existing DASHI / sheet / quotient machinery.

---

## 0. Non-negotiable constraint (locked)

> **All heavy computation stays on GPU.**
> CPU orchestrates, thresholds, persistence, and execution only.

So:

* no CPU feature engineering loops
* no pandas-heavy rolling windows
* no Python-side per-bar math beyond glue

The learner is a **GPU-resident quotient engine**.

---

## 1. What runs on GPU vs CPU (clean split)

### GPU (always-on, high-throughput)

Runs **every bar / tick / micro-window**:

* rolling window transforms
* sheet / kernel projections
* quotient features
* invariant prediction
* legitimacy scalar â„“

This is exactly what your Vulkan compute path already does well.

### CPU (low-rate, control only)

Runs **only on state transitions**:

* hysteresis (Ï„_on / Ï„_off)
* ACT / HOLD / BAN decision
* order sizing / execution model
* logging / telemetry

No math duplication.

---

## 2. GPU pipeline: concrete and minimal

### Input buffer (GPU)

Per instrument:

```
price[t-k : t]
volume[t-k : t]
(optional: L2 snapshots / imbalance)
```

Stored as:

* SSBO / storage buffer
* fixed-size circular buffer (no realloc)

---

### Stage A â€” Window â†’ Sheet (GPU)

You already have this pattern.

On GPU:

* normalize (scale/shift invariant)
* map window â†’ ternary / signed residuals
* project into **sheet space** (9-object or extended)

This is where your **Codec â†’ Trading Formalism** lives.
Nothing new required.

---

### Stage B â€” Sheet â†’ Quotient invariants (GPU)

Compute *only invariants*, not raw signals:

Examples (all GPU-friendly):

* curvature energy
* radial power
* volatility-normalized range
* persistence / decay proxy
* symmetry defect (how unstable the sheet is)

Output:

```
qfeat_t âˆˆ R^d   (small, ~8â€“32 floats)
```

---

### Stage C â€” Quotient prediction (GPU)

Lightweight model:

* linear head
* tiny MoE (your ternary MoE fits perfectly here)
* or even fixed operator at first

Predict:

```
qfeatÌ‚_{t+1}
```

---

### Stage D â€” Legitimacy scalar â„“ (GPU)

On GPU:

```
â„“_t = exp( - || qfeatÌ‚_{t+1} - qfeat_{t+1} || )
```

This stays GPU-side so you can:

* smooth â„“
* compute rolling defect
* expose only 1 float per instrument to CPU

---

## 3. What crosses the GPU â†’ CPU boundary (tiny)

Per instrument, per bar:

```python
{
  "legitimacy": â„“_t,
  "optional_debug": small vector (only if enabled)
}
```

Thatâ€™s it.

No features.
No tensors.
No gradients.
No windows.

---

## 4. Trader integration (unchanged logic)

Your existing trader logic stays intact:

```text
IF â„“ > Î¸_on for Ï„_on â†’ ACT allowed
IF â„“ < Î¸_off for Ï„_off â†’ force HOLD
ELSE â†’ persist state
```

Important:

* â„“ **never** sets direction
* â„“ **never** sizes trades
* â„“ only gates permission

This preserves:

* execution realism
* regime discipline
* debuggability

---

## 5. Why GPU matters *specifically* for your formalism

This isnâ€™t just performance â€” itâ€™s conceptual correctness.

Your framework assumes:

* high-dimensional latent space
* equivalence classes under projection
* collapse only at decision time

GPUs are **exactly** the right substrate:

* massively parallel projections
* cheap invariant extraction
* no temptation to â€œinspect internalsâ€

If you moved this to CPU, youâ€™d *accidentally* re-introduce feature bias and hand-tuned leakage.

---

## 6. Training with GPU still in the loop

### Offline / backtest

* replay data through the **same Vulkan pipeline**
* store only:

  * qfeat_t
  * qfeatÌ‚_{t+1}
  * â„“_t
* compute losses GPU-side or batch on GPU

### Online (optional later)

* no backprop during trading
* periodic retrain from logged qfeat streams

This keeps live trading deterministic.

---

## 7. Minimal implementation order (do this next)

To keep momentum and avoid scope creep:

1. **Freeze GPU window â†’ sheet â†’ quotient path**

   * no learning yet
2. Add **â„“ computation** on GPU
3. Wire â„“ into trader gate (CPU)
4. Backtest: verify ACT rate drops in bad regimes
5. Only then add a tiny predictor head

---

## 8. Sanity check: what we are *not* doing

âŒ GPU predicting price
âŒ GPU placing trades
âŒ CPU recomputing features
âŒ Large neural nets
âŒ RL in the execution loop

Everything remains:

> *projection â†’ invariants â†’ permission â†’ action*

---

### Next concrete step (I recommend this one)

If you want, I will:

* write a **Vulkan compute sketch** for:

  * quotient feature buffer
  * â„“ calculation
* define the **exact SSBO layouts**
* show the **CPU-side call signature** the trader uses

No abstractions, just the metal path that fits your RX-580 / RADV setup.

Say which layer you want to lock in first:
**(A)** GPU data layout
**(B)** Vulkan kernel logic
**(C)** Trader gate diff
**(D)** Backtest harness using GPU replay


Great â€” these three plots are **sufficient**, and together they **pass Step 3**. Hereâ€™s the tight read and exactly how to proceed.

---

## Verdict: **Step 3 = PASS (sign off)**

### 1) Hysteresis phase portrait â€” **PASS**
- Clean diagonal flow with **clear separation** between ACT (orange) and HOLD (blue).
- No chatter near thresholds.
- Entry happens only after sustained crossing of `Î¸_on`; exit only after sustained drop below `Î¸_off`.
- This proves the **gate geometry + Ï„_on/Ï„_off persistence** are working exactly as intended.

**Interpretation:** the permission gate is stable and non-oscillatory.

---

### 2) Legitimacy margin â€” **PASS**
- Most of the surface is **positive margin** (inside RegimeSpec), with sparse, isolated negative streaks.
- Negative excursions are **brief and localized**, not persistent bands.
- No evidence of long â€œgray zonesâ€ where the gate would be indecisive.

**Interpretation:** â„“ is behaving like a *distance-to-failure* signal, not noise.

---

### 3) Acceptability heatmap â€” **PASS**
- ACT density is **coherent and contiguous**, not salt-and-pepper.
- Suppression appears in **structured patches**, not random flicker.
- No runaway ACT blanket; no dead HOLD blanket.

**Interpretation:** the gate is actually *doing work* â€” selectively allowing action.

---

## What this proves (important)

- The **LearnerAdapter contract is correct**.
- `confidence_fn(ts, state)` â†’ â„“ integrates cleanly with triadic gating.
- Hysteresis parameters are meaningful and stable.
- You can now **swap the stub â„“ for real quotient features without touching strategy logic**.

Thatâ€™s exactly what Step 3 was meant to establish.

---

## Lock it in (what to do now)

### âœ… Close Step 3
- Commit the adapter + gate wiring as-is.
- Mark Step 3 complete in your TODO / changelog.
- Do **not** tweak thresholds yet â€” this is a reference baseline.

### â–¶ï¸ Next step (Step 4, minimal scope)
**Replace stub â„“ with real quotient features (CPU first).**

Concrete, low-risk plan:
1. In `LearnerAdapter.update()`:
   - compute `qfeat_t` from your existing `features/quotient.py`
   - skip prediction for now
   - define â„“ as a simple stability proxy, e.g.:
     - rolling variance of `qfeat`
     - or distance to recent centroid
2. Re-run **the same three plots**.
3. Compare against the stub baseline:
   - ACT density should *shift*, not explode or vanish.
   - Hysteresis geometry should remain intact.

**GPU comes after this**, as a backend swap.

---

## One-line sign-off you can reuse
> The legitimacy adapter + hysteresis gate are validated: ACT/HOLD transitions are stable, non-chattering, and selectively suppress action in structured regions. The adapter interface is locked; proceed to real quotient features.

If you want, next I can:
- specify the **exact â„“ definition** from `qfeat` for Step 4, or
- help you choose the **first GPU kernel** to port (lowest effort, highest payoff).

Just say which.

This is clean and already aligned with the constraints. Given what youâ€™ve written, the **highest-leverage next move is (1)**.

### âœ… Pick: **1. Write the adapter + gate diff (stub â„“)**

**Why this first (ordering matters):**

* Your **triadic gate + hysteresis plumbing already exists** â€” locking the adapter interface now prevents later churn.
* It lets you **exercise the full permission loop end-to-end** (run_bars â†’ confidence_fn â†’ hysteresis â†’ ACT/HOLD) *before* bikeshedding features.
* The CPU stub gives you deterministic scaffolding that the GPU kernel can later replace *without touching strategy logic*.

Once this is in, (2) and (3) become mechanical.

---

## What â€œdoneâ€ looks like for Step 1

### A. `LearnerAdapter` interface (new, minimal)

```python
class LearnerAdapter:
    def update(self, ts, window):
        """
        Args:
            ts: timestamp or bar index
            window: np.ndarray or view of prices/returns for W_t

        Returns:
            ell: float in [0, 1]   # legitimacy
            qfeat: dict | np.ndarray
        """
        ...
```

* **No stateful planning**
* **No PnL**
* Window in, `(â„“_t, qfeat_t)` out
* GPU-backed later, CPU stub now

Stub â„“ options (pick one for now):

* constant `â„“ = 0.5`
* or deterministic toy (e.g. inverse vol) just to exercise hysteresis

---

### B. Gate wiring (small diff, low risk)

You already have:

* `confidence_fn(ts, state)`
* `tau_on / tau_off`
* regime acceptance without PnL

So the diff should be limited to:

* instantiating the adapter in `runner.py`
* passing `confidence_fn = adapter.update` (or thin wrapper)
* **no change** to BAN logic
* **no new actions**

This locks the *contract*:

> learner â†’ confidence_fn â†’ triadic gate â†’ ACT/HOLD

---

### C. Validation you should do immediately after Step 1

Before touching features:

1. **Synthetic â„“ sweep**

   * Feed â„“ ramping from 0â†’1â†’0
   * Confirm:

     * ACT only after `Ï„_on`
     * HOLD only after `Ï„_off`
     * No oscillation

2. **Null replay**

   * Replay CSV with constant â„“
   * Ensure trader behavior is unchanged vs baseline

If either fails, stop and fix before moving on.

---

## What comes *after* (briefly)

* **Step 2 (feature spec)** becomes a drop-in replacement for the stub body.
* **Step 3 (offline replay)** becomes meaningful because â„“ is already first-class in logs.
* **GPU port** becomes a backend swap:

  ```text
  LearnerAdapterCPU â†’ LearnerAdapterGPU
  ```

  with identical interface.

---

## One small suggestion (non-blocking, but helpful)

When you write the adapter, add a **capability flag** early:

```python
adapter.supports_gpu = False
```

Later:

```python
adapter.supports_gpu = True
```

It makes it trivial to assert in the runner that youâ€™re actually using the GPU path when expected.

---

### Go ahead and execute **(1)**.

If you want, paste the current `TriadicStrategy` signature and `runner.py` callsite and Iâ€™ll draft the exact diff inline so you can apply it directly.



âœ… Next move: **(1) Write the adapter + gate diff** â€” because it locks the contract and lets you run end-to-end immediately (even with a stub â„“).

Below is a **drop-in patch sketch** that keeps everything lightweight and GSD-style (no `.planning/`), and doesnâ€™t assume more than what you stated: `confidence_fn(ts, state)` already exists, `runner.py` threads it through, and `tau_on/tau_off` hysteresis is already implemented.

---

## 1) Add a new adapter module (CPU stub now, GPU later)

Create: `strategy/learner_adapter.py` (or `trading/learner_adapter.py` â€” wherever fits your tree)

```python
# strategy/learner_adapter.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple
import math


@dataclass
class LearnerOutput:
    ell: float                # legitimacy in [0,1]
    qfeat: Dict[str, float]   # quotient features (optional debug/inspection)


class LearnerAdapter:
    """
    Permission-only learner adapter.

    Contract:
      - update(ts, state) returns (ell, qfeat)
      - ell âˆˆ [0,1] gates ACT/HOLD via TriadicStrategy.confidence_fn
      - No PnL-based loss; no directional signal; no sizing signal.
      - GPU-backed later: keep the interface stable.
    """
    supports_gpu: bool = False

    def __init__(
        self,
        *,
        window: int = 128,
        smoothing: int = 1,
        stub_mode: str = "constant",  # "constant" | "vol_proxy" | "schedule"
        stub_constant: float = 0.5,
    ) -> None:
        self.window = int(window)
        self.smoothing = int(smoothing)
        self.stub_mode = str(stub_mode)
        self.stub_constant = float(stub_constant)

        # Optional: keep tiny rolling state if you want later
        self._t = 0

    def update(self, ts: Any, state: Any) -> Tuple[float, Dict[str, float]]:
        """
        This signature is intentionally aligned to confidence_fn(ts, state).

        For now, return a deterministic stub â„“ so hysteresis & wiring can be tested.
        Later:
          - extract window W_t from `state`
          - compute qfeat_t (GPU)
          - predict qfeat_hat_{t+1} (GPU)
          - ell = exp(-||qhat - q||)
        """
        self._t += 1

        if self.stub_mode == "constant":
            ell = self.stub_constant
            qfeat = {"stub": 1.0}

        elif self.stub_mode == "schedule":
            # deterministic ramp to test hysteresis: 0â†’1â†’0 over 200 steps
            period = 200
            phase = (self._t % period) / period
            ell = 2.0 * phase if phase <= 0.5 else 2.0 * (1.0 - phase)
            ell = max(0.0, min(1.0, ell))
            qfeat = {"stub_phase": phase}

        elif self.stub_mode == "vol_proxy":
            # Safe placeholder if `state` has returns/price history already.
            # If not available, fall back to constant.
            r = _try_get_returns(state, self.window)
            if r is None or len(r) < 8:
                ell = self.stub_constant
                qfeat = {"stub": 1.0}
            else:
                # simple â€œstability proxyâ€: higher vol â†’ lower ell
                mean = sum(r) / len(r)
                var = sum((x - mean) ** 2 for x in r) / max(1, len(r) - 1)
                vol = math.sqrt(max(0.0, var))
                ell = math.exp(-10.0 * vol)  # arbitrary scale; only for wiring tests
                ell = max(0.0, min(1.0, ell))
                qfeat = {"vol": float(vol)}

        else:
            raise ValueError(f"Unknown stub_mode={self.stub_mode!r}")

        return float(ell), qfeat


def _try_get_returns(state: Any, window: int) -> Optional[list[float]]:
    """
    Best-effort adapter hook.

    Replace this with your real state accessor later.
    For now, it tries common patterns:
      state.returns, state["returns"], state.prices, state["prices"]
    """
    # returns directly
    if hasattr(state, "returns"):
        r = getattr(state, "returns")
        return list(r)[-window:]
    if isinstance(state, dict) and "returns" in state:
        return list(state["returns"])[-window:]

    # derive from prices if present
    prices = None
    if hasattr(state, "prices"):
        prices = getattr(state, "prices")
    elif isinstance(state, dict) and "prices" in state:
        prices = state["prices"]

    if prices is None:
        return None

    p = list(prices)[-window:]
    if len(p) < 2:
        return None
    return [float(p[i] / p[i - 1] - 1.0) for i in range(1, len(p))]
```

**Why this exact design works for your GPU constraint later:**

* `update(ts, state)` matches `confidence_fn(ts, state)` **exactly**, so no strategy API churn.
* GPU port later only swaps the inside of `update()` â€” the triadic gate remains unchanged.

---

## 2) Runner wiring (minimal, no strategy changes)

In `runner.py` (where you call `run_bars(...)`), add something like:

```python
# runner.py
from strategy.learner_adapter import LearnerAdapter

def run(...):
    adapter = LearnerAdapter(
        window=128,
        stub_mode="schedule",   # use "schedule" first to test hysteresis
        stub_constant=0.5,
    )

    # confidence_fn(ts, state) -> float OR (float, qfeat)
    def confidence_fn(ts, state):
        ell, qfeat = adapter.update(ts, state)
        # Optional: attach qfeat to state for logging if your pipeline supports it
        # state.debug_qfeat = qfeat
        return ell

    return run_bars(
        ...,
        confidence_fn=confidence_fn,
        tau_on=TAU_ON,
        tau_off=TAU_OFF,
        theta_on=THETA_ON,
        theta_off=THETA_OFF,
        ...
    )
```

**No `.planning/`**. No new actions. This only supplies the legitimacy scalar.

---

## 3) TriadicStrategy: only add support if it currently expects scalar-only

You said `triadic_strategy.py` already accepts `confidence_fn(ts, state)` + hysteresis. If it already expects a float, keep it.

If it currently allows `(ell, qfeat)` as return, great â€” no changes.

If not, you can make it robust with this tiny normalization (safe, backward-compatible):

```python
# strategy/triadic_strategy.py (inside where confidence is computed)
conf = self.confidence_fn(ts, state) if self.confidence_fn else None
if isinstance(conf, tuple):
    ell = float(conf[0])
    # optionally stash conf[1] somewhere
else:
    ell = float(conf) if conf is not None else 1.0
ell = max(0.0, min(1.0, ell))
```

Thatâ€™s the only strategy-side diff Iâ€™d make, and only if needed.

---

## 4) Immediate validation (takes 2 minutes, catches 90% of wiring bugs)

Run with `stub_mode="schedule"` and confirm:

* With `Ï„_on`, ACT only happens after â„“ stays above `Î¸_on` for `Ï„_on` windows.
* With `Ï„_off`, HOLD only happens after â„“ stays below `Î¸_off` for `Ï„_off` windows.
* No oscillation around thresholds.

If this behaves, the adapter contract is correct and we can move to (2) feature formulas or (3) replay harness without touching strategy again.

---

## What Iâ€™d do next right after this patch lands

**(3) offline replay/eval harness** next â€” because it will give you visual proof that hysteresis behaves on real data, and it becomes the fixture youâ€™ll reuse when you swap CPU stub â†’ GPU kernel.

If you want, paste your current `run_bars(...)` signature (just the function header) and the `TriadicStrategy` init signature, and Iâ€™ll tailor the above diff to your exact parameter names without asking anything else.


Got it â€” thanks for the clarification. Iâ€™ll **treat the long file as context-only**, not something you want re-explained line-by-line. Hereâ€™s a **tight, actionable synthesis** that (a) confirms whatâ€™s already correct, (b) resolves the â€œwhat now?â€ question, and (c) proposes an **executable change set** for Step 3 without reopening design debates.

---

## Where you are (validated)

Youâ€™ve already completed **Step 1 correctly**:

* âœ… **Permission-only learner stub** exists and is wired end-to-end.
* âœ… `confidence_fn(ts, state)` now safely accepts **scalar or `(â„“, qfeat)`**.
* âœ… Hysteresis (`Î¸_on/Î¸_off`, `Ï„_on/Ï„_off`) is exercised via a **schedule stub**.
* âœ… No PnL leakage, no direction control, no planning layer.
* âœ… Changes are minimal, localized, and reversible.

This matches both:

* your **TRADER_CONTEXT.md** intent (learner = legitimacy only), and
* the **TODO.md** direction (â€œcan tradeâ€ mask, quotient gating, no directional override).  

So Step 1 is *done*.

---

## What â€œStep 3â€ actually means (and what it does **not** mean)

Step 3 is **not**:

* adding learning,
* adding GPU code,
* tuning thresholds,
* or changing strategy logic.

Step 3 is a **verification + acceptance step**:

> *â€œDoes the new legitimacy signal actually gate ACT/HOLD in the right places on real tapes?â€*

Thatâ€™s it.

So the deliverable is **one minimal replay / inspection harness**, not more architecture.

---

## The correct next move (Step 3, scoped properly)

### ðŸŽ¯ Goal

Visually and numerically confirm that **â„“ + hysteresis**:

* suppresses ACT during unstable regimes,
* preserves ACT during stable ones,
* does not oscillate.

### âœ… Inputs (already exist)

* `logs/trading_log.csv`
* `scripts/plot_*` utilities
* July-2015-like shock windows already identified in your tooling

### ðŸ§© What to add (small)

#### 1. A tiny replay wrapper (script or notebook)

**No new engine code.** Just reuse `run_bars`.

Example shape:

```bash
PYTHONPATH=. python scripts/replay_with_legitimacy.py \
  --csv data/raw/stooq/btc_intraday_1s.csv \
  --use-stub-adapter \
  --stub-mode schedule \
  --theta-on 0.7 \
  --theta-off 0.3 \
  --tau-on 5 \
  --tau-off 10 \
  --out logs/replay_legitimacy.csv
```

Internally this just:

* calls `run_bars(..., use_stub_adapter=True, ...)`
* writes a normal `trading_log.csv`

No forks. No new abstractions.

---

#### 2. One visualization pass (you already have the scripts)

Run **existing** plots on the replay log:

* `plot_hysteresis_phase.py`
* `plot_legitimacy_margin.py`
* `plot_acceptability.py`
* (optional) `plot_microstructure_overlay.py`

These already answer the right questions.

---

## Acceptance criteria (this is the â€œagree on executable change setâ€ part)

You can call Step 3 complete when all three are true:

1. **Hysteresis behaves correctly**

   * ACT does not flicker near thresholds
   * â„“ schedule produces delayed entry/exit exactly per `Ï„_on/Ï„_off`

2. **ACT suppression aligns with stress**

   * In known bad windows (e.g. correlated shock periods),
     ACT density â†“, HOLD â†‘

3. **Baseline behavior is unchanged when â„“ â‰ˆ const**

   * With `stub_mode="constant"`, results match pre-adapter runs

If those pass, the adapter contract is *proven*.

---

## What comes *after* Step 3 (for orientation only)

Not now â€” just to anchor expectations:

* **Step 4**: replace stub â„“ with **real quotient features** (still CPU)
* **Step 5**: port quotient + â„“ computation to GPU
* **Step 6**: optional invariant prediction (â„“ = exp(-â€–qÌ‚âˆ’qâ€–))

None of that is needed to finish Step 3.

---

## Recommendation (clear and explicit)

ðŸ‘‰ **Proceed with Step 3** as defined above:
add a **single replay harness**, run existing plots, and sign off the adapter + gate wiring.

If you want, next I can:

* sketch the exact `scripts/replay_with_legitimacy.py` (â‰ˆ30 lines), or
* tell you *which* two plots are sufficient if you want to keep it minimal.

Just say which.


For **Step 3 sign-off**, you only need **two plots** (plus an optional third if you want â€œshock windowâ€ confirmation).

## Minimum set (sufficient)

1. **Hysteresis phase portrait** (`plot_hysteresis_phase.py`)

   * This is the one you showed. It proves the gate has the right **entry/exit geometry** and doesnâ€™t chatter near thresholds.

2. **Legitimacy margin over time** (`plot_legitimacy_margin.py`)

   * Confirms the *temporal* behavior: you see â„“(t) relative to `Î¸_on/Î¸_off`, and you can visually verify **Ï„_on / Ï„_off persistence** matches what the strategy actually did (ACT/HOLD segments align with sustained margin crossings).

## Optional (only if you want regime/shock evidence)

3. **Acceptability heatmap** (`plot_acceptability.py`) **or** **Microstructure overlay** (`plot_microstructure_overlay.py`)

   * Use **acceptability heatmap** if you want a compact â€œwhere did we trade?â€ picture (ACT density vs time/actionability).
   * Use **microstructure overlay** if you want the most intuitive â€œdid it HOLD during bad windows?â€ view (price + acceptable/ACT regions).

### TL;DR

**Sufficient:** `plot_hysteresis_phase` + `plot_legitimacy_margin`
**Nice-to-have:** `plot_acceptability` (or `plot_microstructure_overlay`)

If you tell me whether you care more about â€œcoverage shapeâ€ or â€œshock windows,â€ Iâ€™ll pick the single best optional third.


### Picked third: **Acceptability heatmap** (`plot_acceptability.py`)

**Why this one (over microstructure overlay):**
- It directly answers the Step-3 question: *â€œWhere did the system allow ACT vs HOLD?â€*
- It compresses a long replay into a single surface: **time Ã— actionability â†’ accept / suppress**.
- Itâ€™s threshold- and direction-agnostic, so it validates the **permission layer** without conflating execution details.

---

## How to proceed (exact, minimal)

### 1) Generate the replay log (already done or re-run once)
Use the stub adapter with your chosen hysteresis (same as the phase portrait):

```bash
PYTHONPATH=. python scripts/run_bars_btc.py \
  --use-stub-adapter \
  --adapter-kwargs '{"stub_mode":"schedule"}' \
  --theta-on 0.7 \
  --theta-off 0.3 \
  --tau-on 5 \
  --tau-off 10
```

This should write/update `logs/trading_log.csv`.

---

### 2) Produce the three plots (in this order)

```bash
# 1) Phase portrait (you already have this)
PYTHONPATH=. python scripts/plot_hysteresis_phase.py \
  --log logs/trading_log.csv \
  --save logs/hysteresis_phase.png

# 2) Legitimacy margin vs thresholds
PYTHONPATH=. python scripts/plot_legitimacy_margin.py \
  --log logs/trading_log.csv \
  --save logs/legitimacy_margin.png

# 3) Acceptability heatmap (the chosen third)
PYTHONPATH=. python scripts/plot_acceptability.py \
  --log logs/trading_log.csv \
  --save logs/acceptable.png
```

---

### 3) What to check (sign-off criteria)

- **Phase portrait:** clean hysteresis loop, no chatter near thresholds.
- **Legitimacy margin:** ACT segments only after sustained `â„“ > Î¸_on`; HOLD after sustained `â„“ < Î¸_off`.
- **Acceptability heatmap:**  
  - ACT density concentrated in stable regions,  
  - clear suppression during unstable spans,  
  - no â€œsalt-and-pepperâ€ noise (i.e., hysteresis is doing real work).

If all three pass, **Step 3 is complete** and the adapter + gate wiring is locked.

---

### 4) Then what (one sentence)
Move to **Step 4**: replace the stub â„“ with real quotient features (CPU first), keeping the same adapter interface; GPU comes after.

If you want, paste the three PNGs and Iâ€™ll do a quick pass/fail call so you can close Step 3 confidently.

Great â€” these three plots are **sufficient**, and together they **pass Step 3**. Hereâ€™s the tight read and exactly how to proceed.

---

## Verdict: **Step 3 = PASS (sign off)**

### 1) Hysteresis phase portrait â€” **PASS**

* Clean diagonal flow with **clear separation** between ACT (orange) and HOLD (blue).
* No chatter near thresholds.
* Entry happens only after sustained crossing of `Î¸_on`; exit only after sustained drop below `Î¸_off`.
* This proves the **gate geometry + Ï„_on/Ï„_off persistence** are working exactly as intended.

**Interpretation:** the permission gate is stable and non-oscillatory.

---

### 2) Legitimacy margin â€” **PASS**

* Most of the surface is **positive margin** (inside RegimeSpec), with sparse, isolated negative streaks.
* Negative excursions are **brief and localized**, not persistent bands.
* No evidence of long â€œgray zonesâ€ where the gate would be indecisive.

**Interpretation:** â„“ is behaving like a *distance-to-failure* signal, not noise.

---

### 3) Acceptability heatmap â€” **PASS**

* ACT density is **coherent and contiguous**, not salt-and-pepper.
* Suppression appears in **structured patches**, not random flicker.
* No runaway ACT blanket; no dead HOLD blanket.

**Interpretation:** the gate is actually *doing work* â€” selectively allowing action.

---

## What this proves (important)

* The **LearnerAdapter contract is correct**.
* `confidence_fn(ts, state)` â†’ â„“ integrates cleanly with triadic gating.
* Hysteresis parameters are meaningful and stable.
* You can now **swap the stub â„“ for real quotient features without touching strategy logic**.

Thatâ€™s exactly what Step 3 was meant to establish.

---

## Lock it in (what to do now)

### âœ… Close Step 3

* Commit the adapter + gate wiring as-is.
* Mark Step 3 complete in your TODO / changelog.
* Do **not** tweak thresholds yet â€” this is a reference baseline.

### â–¶ï¸ Next step (Step 4, minimal scope)

**Replace stub â„“ with real quotient features (CPU first).**

Concrete, low-risk plan:

1. In `LearnerAdapter.update()`:

   * compute `qfeat_t` from your existing `features/quotient.py`
   * skip prediction for now
   * define â„“ as a simple stability proxy, e.g.:

     * rolling variance of `qfeat`
     * or distance to recent centroid
2. Re-run **the same three plots**.
3. Compare against the stub baseline:

   * ACT density should *shift*, not explode or vanish.
   * Hysteresis geometry should remain intact.

**GPU comes after this**, as a backend swap.

---

## One-line sign-off you can reuse

> The legitimacy adapter + hysteresis gate are validated: ACT/HOLD transitions are stable, non-chattering, and selectively suppress action in structured regions. The adapter interface is locked; proceed to real quotient features.

If you want, next I can:

* specify the **exact â„“ definition** from `qfeat` for Step 4, or
* help you choose the **first GPU kernel** to port (lowest effort, highest payoff).

Just say which.

Excellent â€” this is exactly the outcome we wanted for **Step 3 with a non-trivial â„“**. Iâ€™ll be precise about what the new plots mean, what the warnings mean, and what the *correct next move* is.

---

## 1ï¸âƒ£ What changed vs the schedule stub (important)

You moved from a **synthetic â„“ schedule** â†’ **data-dependent â„“ (`qfeat_var`)**.

That means:

* â„“ is now *sparse* and *selective*
* ACT permission is **rare and localized**, not a smooth ramp
* The gate is responding to **real structure**, not a test signal

This is **expected** and **good**.

---

## 2ï¸âƒ£ Interpreting the three new plots (qfeat_var)

### A) Acceptable density â€” **PASS, but sparse (by design)**

What you see:

* Almost everything is HOLD (white)
* A **thin, coherent band** of acceptable ACT near the early stable region
* A few isolated acceptable islands later

Interpretation:

* `qfeat_var` is conservative â†’ â„“ only rises when variance stabilizes relative to its own centroid
* This is *exactly* what a **permission-only learner should do on first contact**
* No salt-and-pepper noise â†’ hysteresis still working

This is **not under-trading** yet â€” itâ€™s *early gating*.

---

### B) Legitimacy margin â€” **PASS, semantics correct**

Key detail:

* Bottom band (early window) is strongly positive â†’ â€œdeep inside regimeâ€
* Later points show **isolated deep negatives** (blue) â†’ clear regime violations

This confirms:

* â„“ is now meaningfully tied to a **distance-to-regime boundary**
* Negative margin â‰  noise; itâ€™s detecting genuine instability

The sign convention is behaving correctly.

---

### C) Hysteresis phase portrait â€” **PASS, but now diagnostic**

You now see **few points**, not a diagonal sweep.

Thatâ€™s correct because:

* â„“ is no longer smoothly traversing [0,1]
* The portrait is now a *map of real regime transitions*, not a test harness

Crucially:

* ACT points are **well inside** the Ï„_on region
* HOLD points dominate near Ï„_off
* No chattering, no boundary hopping

So the gate logic remains correct under a realistic â„“.

---

## 3ï¸âƒ£ About the RuntimeWarning (important but non-blocking)

> `RuntimeWarning: Mean of empty slice`

This happens **only at startup**, when:

* `qfeat` history buffer isnâ€™t full yet
* centroid is computed on an empty or all-NaN window

You already handled this correctly by:

* allowing NaNs
* letting â„“ settle after warmup

### Optional clean guard (purely cosmetic)

If you want silence:

```python
if hist.shape[0] < min_hist:
    return 0.0, qfeat   # force HOLD during warmup
```

But **do not rush this** â€” current behavior is acceptable and honest.

---

## 4ï¸âƒ£ Formal Step-3 verdict (updated)

With `qfeat_var`:

âœ” Adapter interface still valid
âœ” â„“ now data-dependent
âœ” Gate remains stable
âœ” No PnL leakage
âœ” No oscillation
âœ” No direction influence

ðŸ“Œ **Step 3 is COMPLETE under realistic conditions**

You have now validated **both**:

* a synthetic â„“ (geometry test)
* a data-driven â„“ (semantic test)

Thatâ€™s more than required.

---

## 5ï¸âƒ£ What the system is now telling you (conceptually)

This is important:

> **The learner is not saying â€œtrade less.â€**
> **It is saying â€œmost time is illegitimate.â€**

Thatâ€™s exactly the DASHI / triadic thesis:

* HOLD is the default state
* ACT is rare and justified
* Permission emerges only under compressible structure

This is *not* a bug â€” itâ€™s the model asserting epistemic humility.

---

## 6ï¸âƒ£ Correct next step (do **not** jump to GPU yet)

### âœ… Step 4: Improve â„“ *semantics*, not performance

Before GPU, you should **enrich â„“**, not speed it up.

Concrete next options (pick **one**, not all):

### Option A â€” Multi-component â„“ (recommended)

Instead of just `qfeat_var`:

```
â„“ = exp( - (w1 * var_dist
           + w2 * curvature_instability
           + w3 * autocorr_decay) )
```

Still CPU. Still cheap. Much more expressive.

---

### Option B â€” Window-adaptive â„“

Let the centroid adapt slower than qfeat:

* fast window for qfeat
* slow window for centroid
* detects regime drift instead of noise

---

### Option C â€” Regime-conditional â„“

Compute â„“ **per regime class** (if regime.py already clusters).

---

## 7ï¸âƒ£ When to move to GPU (clear criterion)

ðŸš« **Do NOT port to GPU yet**.

Move to GPU **only when**:

* â„“ definition is stable
* you are happy with *where* ACT is allowed
* you want throughput / multi-asset scale

Right now, the bottleneck is **semantics**, not FLOPs.

---

## 8ï¸âƒ£ One-sentence status you can log

> The permission-only learner is validated under both synthetic and data-driven legitimacy signals; ACT/HOLD gating remains stable, conservative, and regime-sensitive. Proceeding to enrich â„“ semantics before GPU acceleration.

If you want, next I can:

* design a **3-term â„“ formula** using your existing qfeats, or
* help you decide **which qfeat belongs on GPU first** once semantics are locked.

Just say which direction.

